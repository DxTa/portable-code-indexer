"""Evaluation prompts and criteria for LLM-as-judge benchmarking.

This module defines the prompts and scoring rubrics used by LLM judges
to evaluate the quality of architectural analysis responses.
"""

from typing import Dict, List
from .architectural_tasks import ArchitecturalTask


def create_analysis_prompt(task: ArchitecturalTask, retrieved_chunks: List[str]) -> str:
    """Create the prompt for the code retrieval tool to answer the architectural question.

    Args:
        task: The architectural task to perform
        retrieved_chunks: Code chunks retrieved by the tool

    Returns:
        Formatted prompt for the tool to generate its answer
    """
    chunks_formatted = "\n\n---\n\n".join(
        f"[Chunk {i + 1}]\n{chunk}" for i, chunk in enumerate(retrieved_chunks)
    )

    return f"""You are analyzing the {task.codebase} codebase to answer an architectural question.

Question: {task.question}

Retrieved Code Chunks:
{chunks_formatted}

Based ONLY on the retrieved code chunks above, provide a detailed answer to the question.
If the retrieved chunks are insufficient to fully answer the question, clearly state what information is missing.

Your answer:"""


def create_judge_prompt(
    task: ArchitecturalTask, tool_response: str, rubric: str = "comprehensive"
) -> str:
    """Create the evaluation prompt for the LLM judge.

    Args:
        task: The architectural task being evaluated
        tool_response: The response generated by the code retrieval tool
        rubric: Scoring rubric to use (comprehensive/quick/strict)

    Returns:
        Formatted prompt for the LLM judge to evaluate the response
    """
    rubrics = {
        "comprehensive": _get_comprehensive_rubric(),
        "quick": _get_quick_rubric(),
        "strict": _get_strict_rubric(),
    }

    rubric_text = rubrics.get(rubric, rubrics["comprehensive"])

    expected_files_list = "\n".join(f"- {f}" for f in task.expected_files)
    expected_concepts_list = "\n".join(f"- {c}" for c in task.expected_concepts)

    return f"""You are evaluating the quality of a code analysis response.

Task Information:
- Question: {task.question}
- Codebase: {task.codebase}
- Difficulty: {task.difficulty}
- Type: {task.task_type}

Expected Coverage:
Files that should be mentioned:
{expected_files_list}

Key concepts that should be covered:
{expected_concepts_list}

Tool Response to Evaluate:
{tool_response}

{rubric_text}

Provide your evaluation in the following JSON format:
{{
  "score": <0-100>,
  "file_coverage": <0-100>,
  "concept_coverage": <0-100>,
  "accuracy": <0-100>,
  "completeness": <0-100>,
  "clarity": <0-100>,
  "reasoning": "<detailed explanation of the score>",
  "missing_elements": ["<list of critical missing information>"],
  "strengths": ["<list of response strengths>"],
  "weaknesses": ["<list of response weaknesses>"]
}}"""


def _get_comprehensive_rubric() -> str:
    """Get the comprehensive evaluation rubric."""
    return """
Evaluation Rubric:

1. File Coverage (0-100):
   - 100: All expected files mentioned and their roles explained
   - 75-99: Most files mentioned, minor omissions
   - 50-74: Some key files mentioned
   - 25-49: Few files mentioned
   - 0-24: Critical files missing

2. Concept Coverage (0-100):
   - 100: All key concepts explained with technical depth
   - 75-99: Most concepts covered, minor gaps
   - 50-74: Core concepts present but incomplete
   - 25-49: Surface-level coverage only
   - 0-24: Critical concepts missing

3. Accuracy (0-100):
   - 100: All statements technically correct
   - 75-99: Mostly correct, minor inaccuracies
   - 50-74: Some correct information, some errors
   - 25-49: Significant errors or misconceptions
   - 0-24: Mostly incorrect or hallucinated

4. Completeness (0-100):
   - 100: Fully answers the question with all necessary detail
   - 75-99: Comprehensive answer, minor details missing
   - 50-74: Partial answer, some aspects unexplored
   - 25-49: Incomplete answer, significant gaps
   - 0-24: Does not adequately address the question

5. Clarity (0-100):
   - 100: Crystal clear explanation, well-structured
   - 75-99: Clear and logical, easy to follow
   - 50-74: Understandable but could be clearer
   - 25-49: Confusing or poorly organized
   - 0-24: Difficult to understand

Overall Score (0-100):
Weighted average: 
- File Coverage: 20%
- Concept Coverage: 25%
- Accuracy: 25%
- Completeness: 20%
- Clarity: 10%
"""


def _get_quick_rubric() -> str:
    """Get the quick evaluation rubric (fewer dimensions)."""
    return """
Quick Evaluation Rubric:

1. Coverage (0-100):
   Does the response cover the expected files and concepts?
   - 100: All expected elements covered thoroughly
   - 50: Partial coverage, key elements present
   - 0: Critical elements missing

2. Quality (0-100):
   Is the response accurate, complete, and clear?
   - 100: Highly accurate, complete, and well-explained
   - 50: Acceptable quality, some issues
   - 0: Poor quality, significant issues

Overall Score: Average of Coverage and Quality
"""


def _get_strict_rubric() -> str:
    """Get the strict evaluation rubric (higher standards)."""
    return """
Strict Evaluation Rubric:

This rubric applies stricter standards for production-grade code analysis.

1. File Coverage (0-100):
   - 100: ALL expected files mentioned with precise roles
   - 90-99: One minor file missing or role unclear
   - 75-89: One key file missing
   - 50-74: Multiple files missing
   - 0-49: Majority of files missing (unacceptable)

2. Concept Coverage (0-100):
   - 100: ALL concepts explained with implementation details
   - 90-99: One minor concept missing
   - 75-89: One key concept missing or shallow
   - 50-74: Multiple concepts missing
   - 0-49: Majority of concepts missing (unacceptable)

3. Accuracy (0-100):
   - 100: Perfect technical accuracy
   - 90-99: One trivial inaccuracy
   - 75-89: One non-critical error
   - 50-74: Significant error (fails review)
   - 0-49: Multiple errors or hallucination (reject)

4. Completeness (0-100):
   - 100: Production-ready answer, could ship to docs
   - 90-99: Minor detail missing
   - 75-89: Needs revision
   - 50-74: Incomplete (send back for rework)
   - 0-49: Unacceptable

5. Clarity (0-100):
   - 100: Technical writing excellence
   - 90-99: Professional quality
   - 75-89: Acceptable but could improve
   - 50-74: Needs significant improvement
   - 0-49: Confusing (rewrite required)

Overall Score:
Same weights as comprehensive rubric, but:
- < 70: Failed (would not pass code review)
- 70-84: Needs revision
- 85-94: Acceptable
- 95-100: Excellent

Minimum acceptable score: 70
"""


def create_comparison_prompt(task: ArchitecturalTask, responses: Dict[str, str]) -> str:
    """Create prompt for comparing multiple tool responses side-by-side.

    Args:
        task: The architectural task being evaluated
        responses: Dictionary mapping tool name to response text

    Returns:
        Formatted prompt for side-by-side comparison
    """
    responses_formatted = "\n\n".join(
        f"=== {tool_name.upper()} ===\n{response}" for tool_name, response in responses.items()
    )

    return f"""You are comparing multiple code analysis tools on the same task.

Task: {task.question}
Codebase: {task.codebase}
Difficulty: {task.difficulty}

Tool Responses:
{responses_formatted}

For each tool, evaluate:
1. Which tool found the most relevant information?
2. Which tool's response is most accurate?
3. Which tool's response is most complete?
4. Which tool would you trust for production use?

Provide a ranking and detailed comparison in JSON format:
{{
  "ranking": ["<tool_name>", ...],  // Best to worst
  "scores": {{
    "<tool_name>": {{
      "relevance": <0-100>,
      "accuracy": <0-100>,
      "completeness": <0-100>,
      "overall": <0-100>
    }},
    ...
  }},
  "analysis": "<detailed comparison explaining the ranking>",
  "recommendation": "<which tool would you recommend and why>"
}}"""


def create_retrieval_quality_prompt(
    task: ArchitecturalTask, retrieved_files: List[str], chunk_count: int
) -> str:
    """Create prompt for evaluating retrieval quality before analysis.

    Args:
        task: The architectural task being evaluated
        retrieved_files: List of file paths retrieved
        chunk_count: Total number of chunks retrieved

    Returns:
        Formatted prompt for evaluating retrieval quality
    """
    files_list = "\n".join(f"- {f}" for f in retrieved_files)
    expected_files_list = "\n".join(f"- {f}" for f in task.expected_files)

    return f"""You are evaluating the quality of code retrieval BEFORE seeing the analysis.

Task: {task.question}

Expected Files (Ground Truth):
{expected_files_list}

Retrieved Files ({len(retrieved_files)} files, {chunk_count} chunks):
{files_list}

Evaluate the retrieval quality:

1. Precision: How many retrieved files are relevant to the task?
2. Recall: How many expected files were successfully retrieved?
3. Noise: Are there irrelevant files that would confuse the analysis?
4. Sufficiency: Is this enough information to answer the question?

Provide your evaluation in JSON format:
{{
  "precision_score": <0-100>,
  "recall_score": <0-100>,
  "noise_score": <0-100>,
  "sufficiency_score": <0-100>,
  "overall_retrieval_quality": <0-100>,
  "missing_critical_files": ["<list of critical files not retrieved>"],
  "irrelevant_files": ["<list of retrieved but irrelevant files>"],
  "analysis": "<detailed explanation>"
}}"""
